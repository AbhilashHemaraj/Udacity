{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the set of labels into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perfect_labeler</th>\n",
       "      <th>radiologist</th>\n",
       "      <th>algorithm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>benign</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>benign</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>benign</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>benign</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>benign</td>\n",
       "      <td>cancer</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    perfect_labeler radiologist  algorithm\n",
       "144          benign      cancer       0.06\n",
       "145          benign      cancer       0.98\n",
       "146          benign      cancer       0.77\n",
       "147          benign      cancer       0.03\n",
       "148          benign      cancer       0.02"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start with assessing the radiologist's performance:\n",
    "* Assess the _accuracy_ of the radiologist by just looking at the percent of cases that they correctly labeled\n",
    "* Next, look at the true positive and true negative rates of the radiologist by generating a _confusion matrix_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "radiologist_accuracy = sum(labels.perfect_labeler == labels.radiologist)/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8993288590604027"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radiologist_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 25,   4],\n",
       "       [ 11, 109]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(labels.perfect_labeler.values,labels.radiologist.values,labels=[\"cancer\",\"benign\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now look at the algorithm's performance compared to the perfect labeler:\n",
    "* Since the algorithm doesn't create a binary label, it instead returns a _probability_ of cancer, choose a probability cut-off to use for the algorithm's labeling of cancer vs. bening. _(Hint: 0.5 is a reasonable starting place)_\n",
    "* Start with assessing _accuracy_ again here\n",
    "* Generate a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[\"algorithm_prediction\"] = np.arange(0, len(labels))\n",
    "\n",
    "def performance_of_algorithm(X, y):\n",
    "    for i in range(0, len(X)):\n",
    "        if X.algorithm[i] >= 0.5:\n",
    "            labels[\"algorithm_prediction\"][i] = \"cancer\" # creating labels\n",
    "        else:\n",
    "            labels[\"algorithm_prediction\"][i] = \"benign\"\n",
    "    \n",
    "    algorithm_accuracy = sum(y == X.algorithm_prediction)/len(labels) # assessing accuracy\n",
    "    print(algorithm_accuracy) \n",
    "    \n",
    "    print(confusion_matrix(y.values, X.algorithm_prediction.values, labels = [\"cancer\", \"benign\"])) # confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8859060402684564\n",
      "[[ 21   8]\n",
      " [  9 111]]\n"
     ]
    }
   ],
   "source": [
    "performance_of_algorithm(labels, labels.perfect_labeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens now if you change the threshold cut-off for your algorithm's classification to 0.4? What if you raise it to 0.6? How do accuracy, fp, fn, tp, and tn change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8590604026845637\n",
      "[[ 25   4]\n",
      " [ 17 103]]\n"
     ]
    }
   ],
   "source": [
    "# With cutoff as 0.4, The accuracy dips.\n",
    "def performance_of_algorithm(X, y):\n",
    "    for i in range(0, len(X)):\n",
    "        if X.algorithm[i] >= 0.4:\n",
    "            labels[\"algorithm_prediction\"][i] = \"cancer\" # creating labels\n",
    "        else:\n",
    "            labels[\"algorithm_prediction\"][i] = \"benign\"\n",
    "    \n",
    "    algorithm_accuracy = sum(y == X.algorithm_prediction)/len(labels) # assessing accuracy\n",
    "    print(algorithm_accuracy) \n",
    "    \n",
    "    print(confusion_matrix(y.values, X.algorithm_prediction.values, labels = [\"cancer\", \"benign\"])) # confusion matrix\n",
    "\n",
    "\n",
    "performance_of_algorithm(labels, labels.perfect_labeler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9060402684563759\n",
      "[[ 20   9]\n",
      " [  5 115]]\n"
     ]
    }
   ],
   "source": [
    "# With cutoff as 0.6, The accuracy increases.\n",
    "def performance_of_algorithm(X, y):\n",
    "    for i in range(0, len(X)):\n",
    "        if X.algorithm[i] >= 0.6:\n",
    "            labels[\"algorithm_prediction\"][i] = \"cancer\" # creating labels\n",
    "        else:\n",
    "            labels[\"algorithm_prediction\"][i] = \"benign\"\n",
    "    \n",
    "    algorithm_accuracy = sum(y == X.algorithm_prediction)/len(labels) # assessing accuracy\n",
    "    print(algorithm_accuracy) \n",
    "    \n",
    "    print(confusion_matrix(y.values, X.algorithm_prediction.values, labels = [\"cancer\", \"benign\"])) # confusion matrix\n",
    "\n",
    "\n",
    "performance_of_algorithm(labels, labels.perfect_labeler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, let's compare our algorithm to the radiologist\n",
    "* A \"perfect labeler\" might not exist in the real world, and in fact, if often does not\n",
    "* In AI for medical imaging, using a radiologist's labels as our \"true\" label is often the standard of practice, and algorithm performance is judged in both an academic setting as well as in the regulated industry landscape based on performance against an expert human\n",
    "\n",
    "* Repeat the steps above using a set threshold for your algorithm (again, 0.5 is perfectly reasonable) but now computing accuracy, tp, tn, fp, fn against the radiologist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the algorithm against the radiologist's labels =  0.8791946308724832\n",
      "\n",
      "The confusion matrix : \n",
      " [[ 23  13]\n",
      " [  5 108]]\n",
      "\n",
      "True Positive =  23\n",
      "\n",
      "True Negative =  108\n",
      "\n",
      "False Positive =  23\n",
      "\n",
      "False Negative =  23\n"
     ]
    }
   ],
   "source": [
    "# With cutoff as 0.55, The accuracy increases.\n",
    "def performance_of_algorithm(X, y):\n",
    "    for i in range(0, len(X)):\n",
    "        if X.algorithm[i] >= 0.55:\n",
    "            labels[\"algorithm_prediction\"][i] = \"cancer\" # creating labels\n",
    "        else:\n",
    "            labels[\"algorithm_prediction\"][i] = \"benign\"\n",
    "    \n",
    "    algorithm_accuracy = sum(y == X.algorithm_prediction)/len(labels) # assessing accuracy\n",
    "    print(\"Accuracy of the algorithm against the radiologist's labels = \", algorithm_accuracy) \n",
    "    \n",
    "    c_matrix = confusion_matrix(y.values, X.algorithm_prediction.values, labels = [\"cancer\", \"benign\"])\n",
    "    print(\"\\nThe confusion matrix : \\n\", c_matrix) # confusion matrix\n",
    "    \n",
    "    tp = c_matrix[0][0]\n",
    "    print(\"\\nTrue Positive = \", tp)\n",
    "    \n",
    "    tn = c_matrix[1][1]\n",
    "    print(\"\\nTrue Negative = \", tn)\n",
    "    \n",
    "    fp = c_matrix[1][0]\n",
    "    print(\"\\nFalse Positive = \", tp)\n",
    "    \n",
    "    fn = c_matrix[0][1]\n",
    "    print(\"\\nFalse Negative = \", tp)\n",
    "    \n",
    "performance_of_algorithm(labels, labels.radiologist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection: \n",
    "* In the above exercise you assess performances of a human as well as of an algorithm against a 'perfect labeler' and also against each other. \n",
    "* Does accuracy seem like the appropriate statistic to use when evaluating these labels? Why or why not? \n",
    "* In what clinical settings does it seem more or less acceptable to have a high level of FNs? FPs? \n",
    "* How did changing the threshold on the algorithm performance change the different performance statistics? \n",
    "* How did your opinion of the algorithm's performance change when you started comparing it to a radiologist instead of the perfect labeler? What does this mean for a real-world scenario when a perfect labeler doesn't exist, and we only have a radiologist's read to base our performance on? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Does accuracy seem like the appropriate statistic to use when evaluating these labels? Why or why not? \n",
    "\"\"\" \n",
    "\n",
    "I believe that eventhough\n",
    "accuracy metric shows how good your model performed overall, it is not\n",
    "an indicator of what aspects the model performs good or bad. \n",
    "Knowing specifically if the the model has higher specificity or sensitivity \n",
    "would give a better picture of the performance of the model.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# In what clinical settings does it seem more or less acceptable to have a high level of FNs? FPs? \n",
    "\"\"\"\n",
    "It is less acceptable to have high False positives and False Negative wherein the clinical situation is to rule the patient \n",
    "having a disease or not. FOr e.g.cancer.(increased sensitivity) and also (decreased specificity).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# How did changing the threshold on the algorithm performance change the different performance statistics? \n",
    "\"\"\"\n",
    "Changing the threshold not only saw a fluctuation in the accuracy achieved but it affected certain other measures such as increased recall, etc.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# How did your opinion of the algorithm's performance change when you started comparing it to a radiologist instead of the perfect labeler? What does this mean for a real-world scenario when a perfect labeler doesn't exist, and we only have a radiologist's read to base our performance on? \n",
    "\"\"\"\n",
    "Knowing that even the most experienced radiologist could make a misintrepretation of an image, our algorithm did overall well. \n",
    "I believe given that there is no perfect labeler in the real world, we would have to foocus on generalization of our model.\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
